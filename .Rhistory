# Representation of normal quantiles for the column GrLivArea
par(mfrow = c(1, 1))
# Define the column name
j0 <- "GrLivArea"
# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)
# Create a quantile-quantile plot
qqnorm(data[, j0], main = paste("Q-Q Plot of", j0), pch = 19, col = 2)
qqline(data[, j0])
# Print the row corresponding to GrLivArea
print(subset(result, variable == "GrLivArea"))
data_tidy <- melt(data, value.name = "value")
# Use the aggregate function with the correct FUN argument
result <- aggregate(value ~ variable, data = data_tidy,
FUN = function(x) shapiro.test(x)$p.value)
# Print the row corresponding to GrLivArea
print(subset(result, variable == "GrLivArea"))
# Classical numeric descriptive analysis
describe(X1stFlrSF)
# Histogram and density plots
p1<-ggplot(data,aes(x=X1stFlrSF))+geom_density()+
labs(title = "Density of X1stFlrSF",x="X1stFlrSF",y="Values")
p2<-ggplot(data,aes(x=X1stFlrSF))+geom_histogram()+
labs(title = "Histogram of X1stFlrSF",x="X1stFlrSF",y="Values")
# This function controls the graphical output device
ggarrange(p1,p2, nrow=1, common.legend = FALSE)
# Missing Values
na_counts <- sum(is.na(data$X1stFlrSF))
print(na_counts)
print(na_counts/nrow(data))
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = X1stFlrSF)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() +
labs(title = "Boxplot of X1stFlrSF", x = "Values", y = "")
ggarrange(p1, nrow=1, common.legend = FALSE)
# Count outliers
outliers <- boxplot.stats(data$X1stFlrSF)$out
# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")
# Identify rows with outliers
outlier_rows <- which(data$X1stFlrSF %in% outliers)
# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")
# Remove rows with outliers
data <- data[-outlier_rows, ]
# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")
# Histogram representation of the column X1stFlrSF
par(mfcol = c(1, 1))
# Define the column name
j0 <- "X1stFlrSF"
# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)
# Create a histogram for the entire column
hist(data[, j0], prob = TRUE, col = grey(0.8), main = paste("Histogram of", j0), xlab = j0)
# Overlay normal distribution curve
lines(x0, dnorm(x0, mean(data[, j0], na.rm = TRUE), sd(data[, j0], na.rm = TRUE)), col = "blue", lwd = 2)
# Representation of normal quantiles for the column X1stFlrSF
par(mfrow = c(1, 1))
# Define the column name
j0 <- "X1stFlrSF"
# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)
# Create a quantile-quantile plot
qqnorm(data[, j0], main = paste("Q-Q Plot of", j0), pch = 19, col = 2)
qqline(data[, j0])
data_tidy <- melt(data, value.name = "value")
# Use the aggregate function with the correct FUN argument
result <- aggregate(value ~ variable, data = data_tidy,
FUN = function(x) shapiro.test(x)$p.value)
# Print the row corresponding to X1stFlrSF
print(subset(result, variable == "X1stFlrSF"))
# Classical numeric descriptive analysis
describe(X2ndFlrSF)
# Histogram and density plots
p1<-ggplot(data,aes(x=X2ndFlrSF))+geom_density()+
labs(title = "Density of X2ndFlrSF",x="X2ndFlrSF",y="Values")
p2<-ggplot(data,aes(x=X2ndFlrSF))+geom_histogram()+
labs(title = "Histogram of X2ndFlrSF",x="X2ndFlrSF",y="Values")
# This function controls the graphical output device
ggarrange(p1,p2, nrow=1, common.legend = FALSE)
# Missing Values
na_counts <- sum(is.na(data$X2ndFlrSF))
print(na_counts)
print(na_counts/nrow(data))
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = X2ndFlrSF)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() +
labs(title = "Boxplot of X2ndFlrSF", x = "Values", y = "")
ggarrange(p1, nrow=1, common.legend = FALSE)
# Count outliers
outliers <- boxplot.stats(data$X2ndFlrSF)$out
# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")
# Identify rows with outliers
outlier_rows <- which(data$X2ndFlrSF %in% outliers)
# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")
# Histogram representation of the column X1stFlrSF
par(mfcol = c(1, 1))
# Define the column name
j0 <- "X2ndFlrSF"
# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)
# Create a histogram for the entire column
hist(data[, j0], prob = TRUE, col = grey(0.8), main = paste("Histogram of", j0), xlab = j0)
# Overlay normal distribution curve
lines(x0, dnorm(x0, mean(data[, j0], na.rm = TRUE), sd(data[, j0], na.rm = TRUE)), col = "blue", lwd = 2)
# Representation of normal quantiles for the column X2ndFlrSF
par(mfrow = c(1, 1))
# Define the column name
j0 <- "X2ndFlrSF"
# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)
# Create a quantile-quantile plot
qqnorm(data[, j0], main = paste("Q-Q Plot of", j0), pch = 19, col = 2)
qqline(data[, j0])
# Melt the data
data_tidy <- melt(data, value.name = "value")
result <- aggregate(value ~ variable, data = data_tidy,
FUN = function(x) shapiro.test(x)$p.value)
# Print the row corresponding to X2ndFlrSF
print(subset(result, variable == "X2ndFlrSF"))
print(result)
# Classical numeric descriptive analysis
describe(YrSold)
# Histogram and density plots
p1<-ggplot(data,aes(x=YrSold))+geom_density()+
labs(title = "Density of YrSold",x="YrSold",y="Values")
p2<-ggplot(data,aes(x=YrSold))+geom_histogram()+
labs(title = "Histogram of YrSold",x="YrSold",y="Values")
# This function controls the graphical output device
ggarrange(p1,p2, nrow=1, common.legend = FALSE)
# Missing Values
na_counts <- sum(is.na(data$YrSold))
print(na_counts)
print(na_counts/nrow(data))
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = YrSold)) +
geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
coord_flip() +
labs(title = "Boxplot of YrSold", x = "Values", y = "")
ggarrange(p1, nrow=1, common.legend = FALSE)
# Count outliers
outliers <- boxplot.stats(data$YrSold)$out
# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")
# Identify rows with outliers
outlier_rows <- which(data$YrSold %in% outliers)
# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")
# Basic descriptive statistics
freq(SaleType)
# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = SaleType))+geom_bar()+
coord_polar("y")+labs(x="SaleType",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = SaleType))+geom_bar()+
labs(x="SaleType",y="%")
# This function controls the graphical output device
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)
# Basic descriptive statistics
freq(SaleCondition)
# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = SaleCondition))+geom_bar()+
coord_polar("y")+labs(x="SaleCondition",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = SaleCondition))+geom_bar()+
labs(x="SaleCondition",y="%")
# This function controls the graphical output device
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)
# Classical numeric descriptive analysis
describe(SalePrice)
# Histogram and density plots
p1<-ggplot(data,aes(x=SalePrice))+geom_density()+
labs(title = "Density of SalePrice",x="SalePrice",y="Values")
p2<-ggplot(data,aes(x=SalePrice))+geom_histogram()+
labs(title = "Histogram of SalePrice",x="SalePrice",y="Values")
# This function controls the graphical output device
ggarrange(p1,p2, nrow=1, common.legend = FALSE)
# Classical numeric descriptive analysis
describe(log(SalePrice))
# Histogram and density plots
p1<-ggplot(data,aes(x=log(SalePrice)))+geom_density()+
labs(title = "Density of SalePrice",x="SalePrice",y="Values")
p2<-ggplot(data,aes(x=log(SalePrice)))+geom_histogram()+
labs(title = "Histogram of SalePrice",x="SalePrice",y="Values")
# This function controls the graphical output device
ggarrange(p1,p2, nrow=1, common.legend = FALSE)
# Split the target variable into two categories of equal frequency
data$PriceCategory <- ntile(data$SalePrice, 2)
# Name each category
data$PriceCategory <- ifelse(data$PriceCategory == 1, "Cheap", "Expensive")
# Numeric descriptive analysis and graphics
describe(data$SalePrice)
p1 <- ggplot(data, aes(x = SalePrice)) +
geom_density() +
labs(title = "Density of SalePrice", x = "SalePrice", y = "Density")
p2 <- ggplot(data, aes(x = SalePrice)) +
geom_histogram(bins = 30) +
labs(title = "Histogram of SalePrice", x = "SalePrice", y = "Count")
ggarrange(p1, p2, nrow = 1, common.legend = FALSE)
# Verify the new variable
table(data$PriceCategory)
# Drop the SalePrice variable
data <- select(data, -SalePrice)
# To verify that the column has been removed, you can view the structure of the dataframe
str(data)
# Basic descriptive statistics
freq(data$PriceCategory)
# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = PriceCategory))+geom_bar()+
coord_polar("y")+labs(x="PriceCategory",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = PriceCategory))+geom_bar()+
labs(x="PriceCategory",y="%")
# This function controls the graphical output device
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)
###############################
# Correlation at sample level #
###############################
# Select only numeric columns
numeric_data <- data[sapply(data, is.numeric)]
# Are the variables correlated at sample level?
correlation_matrix <- cor(numeric_data)
# Display the first 6x6 portion of the correlation matrix
correlation_matrix
# Compute the determinant of the correlation matrix (for full matrix)
det(correlation_matrix)
###################################
# Correlation at population level #
###################################
# Bartlett's sphericity test:
# This test checks whether the correlations are significantly different from 0
# The null hypothesis is H_0; det(R)=1 means the variables are uncorrelated
# R denotes the correlation matrix
# cortest.bartlett function in the package pysch performs this test
# This function works with standardized data.
# Standardization
numeric_data_scale<-scale(numeric_data)
# Bartlett's sphericity test
cortest.bartlett(cor(numeric_data_scale))
# Polychoric correlation matrix
poly_cor<-hetcor(numeric_data)$correlations
ggcorrplot(poly_cor, type="lower",hc.order=T)
# Another interesting visual representation is the following
corrplot(cor(numeric_data), order = "hclust", tl.col='black', tl.cex=1)
# The 'prcomp' function in the base R package performs this analysis
# Parameters 'scale' and 'center' are set to TRUE to consider standardized data
PCA<-prcomp(numeric_data, scale=T, center = T)
# The field 'rotation' of the 'PCA' object is a matrix
# Its columns are the coefficients of the principal components
# Indicates the weight of each variable in the corresponding principal component
PCA$rotation
# Standard deviations of each principal component
PCA$sdev
# The function 'summary' applied to the 'PCA' object provides relevant information
# - Standard deviations of each principal component
# - Proportion of variance explained and cummulative variance
summary(PCA)
# The following graph shows the proportion of explained variance
Explained_variance <- PCA$sdev^2 / sum(PCA$sdev^2)
p1<-ggplot(data = data.frame(Explained_variance, pc = 1:10),
aes(x = pc, y = Explained_variance, fill=Explained_variance )) +
geom_col(width = 0.3) + scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Principal component", y= "Proportion of variance")
# The following graph shows the proportion of cumulative explained variance
Cummulative_variance<-cumsum(Explained_variance)
p2<-ggplot( data = data.frame(Cummulative_variance, pc = 1:10),
aes(x = pc, y = Cummulative_variance ,fill=Cummulative_variance )) +
geom_col(width = 0.5) +  scale_y_continuous(limits = c(0,1)) + theme_bw() +
labs(x = "Principal component",
y = "Proportion of cumulative variance")
p1
p2
fviz_eig(PCA)
#######################
# Rule of Abdi et al. #
#######################
# Variances
PCA$sdev^2
# Average of variances
mean(PCA$sdev^2)
# These graphical outputs show the projection of the variables in two dimensions
# Display the weight of the variable in the direction of the principal component
# Projection of variables on the first and second principal components
p1 <- fviz_pca_var(PCA, axes = c(1, 2), repel = TRUE, col.var = "cos2",
legend.title = "Distance", title = "Variables (PC1 vs PC2)") + theme_bw()
# Projection on the first and third principal components
p2 <- fviz_pca_var(PCA, axes = c(1, 3), repel = TRUE, col.var = "cos2",
legend.title = "Distance", title = "Variables (PC1 vs PC3)") + theme_bw()
# Projection on the first and fourth principal components
p3 <- fviz_pca_var(PCA, axes = c(1, 4), repel = TRUE, col.var = "cos2",
legend.title = "Distance", title = "Variables (PC1 vs PC4)") + theme_bw()
# Projection on the second and third principal components
p4 <- fviz_pca_var(PCA, axes = c(2, 3), repel = TRUE, col.var = "cos2",
legend.title = "Distance", title = "Variables (PC2 vs PC3)") + theme_bw()
# Projection on the second and fourth principal components
p5 <- fviz_pca_var(PCA, axes = c(2, 4), repel = TRUE, col.var = "cos2",
legend.title = "Distance", title = "Variables (PC2 vs PC4)") + theme_bw()
# Projection on the third and fourth principal components
p6 <- fviz_pca_var(PCA, axes = c(3, 4), repel = TRUE, col.var = "cos2",
legend.title = "Distance", title = "Variables (PC3 vs PC4)") + theme_bw()
# Displaying graphics
p1
p2
p3
p4
p5
p6
# It is also possible to represent the observations
# As well as identify with colors those observations that explain the greatest
# variance of the principal components
p1<-fviz_pca_ind(PCA,axes = c(1,2),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()
p2<-fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()
p3<-fviz_pca_ind(PCA,axes=c(1,4),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()
p4<-fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()
p5<-fviz_pca_ind(PCA,axes=c(2,4),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()
p6<-fviz_pca_ind(PCA,axes=c(3,4),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()
# Displaying graphics
p1
p2
p3
p4
p5
p6
# Joint representation of variables and observations
# Relates the possible relationships between the contributions of the records
# to the variances of the components and the weight of the variables in each
# principal component
p1<-fviz_pca(PCA,axes=c(1,2),alpha.ind ="contrib", col.var = "cos2",
col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE, legend.title="Distancia")+theme_bw()
p2<-fviz_pca(PCA,axes=c(1,3),alpha.ind ="contrib",
col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE, legend.title="Distancia")+theme_bw()
p3<-fviz_pca(PCA,axes=c(1,4),alpha.ind ="contrib",
col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE, legend.title="Distancia")+theme_bw()
p4<-fviz_pca(PCA,axes=c(2,3),alpha.ind ="contrib",
col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE, legend.title="Distancia")+theme_bw()
p5<-fviz_pca(PCA,axes=c(2,4),alpha.ind ="contrib",
col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE, legend.title="Distancia")+theme_bw()
p6<-fviz_pca(PCA,axes=c(3,4),alpha.ind ="contrib",
col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE, legend.title="Distancia")+theme_bw()
# Displaying graphics
p1
p2
p3
p4
p5
p6
head(PCA$x)
# Scree plot
scree(poly_cor)
#Parallel analysis
fa.parallel(poly_cor,n.obs=100,fa="fa",fm="ml")
modelo_varimax<-fa(poly_cor,nfactors = 3,rotate = "varimax",
fa="mle")
# The rotated factorial matrix is shown
print(modelo_varimax$loadings,cut=0)
fa.diagram(modelo_varimax)
# Remember that package 'stats' is required for 'factanal' function
# This function only performs the mle method
FA<-factanal(numeric_data,factors=3, rotation="varimax")
FA$loadings
# Royston multivariate normality test
royston_test <- MVN::mvn(data = numeric_data, mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality
# Henze-Zirkler multivariate normality test
hz_test <- MVN::mvn(data = numeric_data, mvnTest = "hz")
hz_test$multivariateNormality
# The response variable has to be an object of the class 'factor' of R language
data$PriceCategory<-as.factor(data$PriceCategory)
set.seed(3)
# Partitioning the dataset: training (80%) + test (20%)
trainIndex<-createDataPartition(data$PriceCategory,p=0.80)$Resample
datos_train<-data[trainIndex,]
datos_test<-data[-trainIndex,]
datos_train
datos_test
modelo_lda <- lda(formula = PriceCategory ~ LotFrontage + MSSubClass + MSZoning + ExterQual + ExterCond + SaleType + SaleCondition + LotArea + OverallQual + OverallCond + YearBuilt + GrLivArea + X1stFlrSF + X2ndFlrSF + YrSold ,data = datos_train)
modelo_lda
nuevas_observaciones <- datos_test
predict(object = modelo_lda, newdata = nuevas_observaciones)
pred <- predict(object = modelo_lda, newdata = datos_test)
confusionmatrix(datos_test$PriceCategory, pred$class)
# Classification error percentage
trainig_error <- mean(datos_test$PriceCategory != pred$class) * 100
paste("trainig_error=", trainig_error, "%")
# Asegúrate de obtener las probabilidades predichas
pred <- predict(object = modelo_lda, newdata = datos_test, type = "response")
# Calcula la curva ROC usando las probabilidades y la clase verdadera
roc_curve <- roc(datos_test$PriceCategory, as.numeric(pred$posterior[,2]), levels = c("Cheap", "Expensive"))
# Dibuja la curva ROC
plot(roc_curve, main="Curva ROC para LDA", col="#1c61b6")
# Agrega línea diagonal (clasificador aleatorio)
abline(0, 1, lty=2, col = "red")
# Opcional: Calcula el área bajo la curva (AUC)
auc(roc_curve)
# Calcula la matriz de confusión
conf_mat <- confusionMatrix(pred$class, datos_test$PriceCategory)
# Extrae los valores de TP, FP, TN, FN
TP <- conf_mat$table[2,2]
FP <- conf_mat$table[1,2]
TN <- conf_mat$table[1,1]
FN <- conf_mat$table[2,1]
# Calcula las métricas
PPV <- TP / (TP + FP)  # Positive Predictive Value
TPR <- TP / (TP + FN)  # True Positive Rate o Sensitivity
TNR <- TN / (TN + FP)  # True Negative Rate o Specificity
F1 <- 2 * PPV * TPR / (PPV + TPR)  # F1 Score
G_mean <- sqrt(TPR * TNR)  # G-mean
Accuracy <- (TP + TN) / (TP + FP + FN + TN)  # Accuracy
# Calcula AUC
roc_curve <- roc(datos_test$PriceCategory, as.numeric(pred$posterior[,2]), levels = c("Cheap", "Expensive"))
AUC <- auc(roc_curve)
# Imprime las métricas
cat("PPV:", PPV, "\n",
"TPR:", TPR, "\n",
"TNR:", TNR, "\n",
"F1-score:", F1, "\n",
"G-mean:", G_mean, "\n",
"Accuracy:", Accuracy, "\n",
"AUC:", AUC, "\n")
modelo_qda <- qda(PriceCategory ~ LotFrontage + LotArea + OverallQual + OverallCond + YearBuilt + GrLivArea + X1stFlrSF + X2ndFlrSF + YrSold ,data = datos_train)
modelo_qda
nuevas_observaciones <- datos_test
predict(object = modelo_qda, newdata = nuevas_observaciones)
pred <- predict(object = modelo_qda, newdata = datos_test)
confusionmatrix(datos_test$PriceCategory, pred$class)
# Classification error percentage
trainig_error <- mean(datos_test$PriceCategory != pred$class) * 100
paste("trainig_error=", trainig_error, "%")
# Asegúrate de obtener las probabilidades predichas
pred <- predict(object = modelo_qda, newdata = datos_test, type = "response")
# Calcula la curva ROC usando las probabilidades y la clase verdadera
roc_curve <- roc(datos_test$PriceCategory, as.numeric(pred$posterior[,2]), levels = c("Cheap", "Expensive"))
# Dibuja la curva ROC
plot(roc_curve, main="Curva ROC para LDA", col="#1c61b6")
# Agrega línea diagonal (clasificador aleatorio)
abline(0, 1, lty=2, col = "red")
# Opcional: Calcula el área bajo la curva (AUC)
auc(roc_curve)
# Calcula la matriz de confusión
conf_mat <- confusionMatrix(pred$class, datos_test$PriceCategory)
# Extrae los valores de TP, FP, TN, FN
TP <- conf_mat$table[2,2]
FP <- conf_mat$table[1,2]
TN <- conf_mat$table[1,1]
FN <- conf_mat$table[2,1]
# Calcula las métricas
PPV <- TP / (TP + FP)  # Positive Predictive Value
TPR <- TP / (TP + FN)  # True Positive Rate o Sensitivity
TNR <- TN / (TN + FP)  # True Negative Rate o Specificity
F1 <- 2 * PPV * TPR / (PPV + TPR)  # F1 Score
G_mean <- sqrt(TPR * TNR)  # G-mean
Accuracy <- (TP + TN) / (TP + FP + FN + TN)  # Accuracy
# Calcula AUC
roc_curve <- roc(datos_test$PriceCategory, as.numeric(pred$posterior[,2]), levels = c("Cheap", "Expensive"))
AUC <- auc(roc_curve)
# Imprime las métricas
cat("PPV:", PPV, "\n",
"TPR:", TPR, "\n",
"TNR:", TNR, "\n",
"F1-score:", F1, "\n",
"G-mean:", G_mean, "\n",
"Accuracy:", Accuracy, "\n",
"AUC:", AUC, "\n")
distance<- get_dist(numeric_data)
fviz_dist(distance, gradient = list(low ="yellow", mid = "white", high = "brown"))
dendrogram <- hclust(dist(numeric_data, method = 'euclidean'), method = 'ward.D')
ggdendrogram(dendrogram, rotate = FALSE, labels = FALSE, theme_dendro = TRUE) +
labs(title = "Dendrograma")
k2 <- kmeans(numeric_data, centers = 2, nstart = 25)
# Displaying all the fields of the object k2
str(k2)
k2
fviz_cluster(k2,data=numeric_data)
set.seed(123)
fviz_nbclust(numeric_data, kmeans, method = "wss")
set.seed(123)
fviz_nbclust(numeric_data, kmeans, method = "silhouette")
set.seed(123)
gap_stat <- clusGap(numeric_data, FUN = kmeans, nstart = 25,K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
# Agregar las etiquetas de cluster al conjunto de datos original (asegúrate de que las filas coincidan)
data$Cluster <- k2$cluster
# Crear una tabla cruzada para comparar los clusters con PriceCategory
table(data$PriceCategory, data$Cluster)
adjusted_rand_index <- adjustedRandIndex(data$PriceCategory, data$Cluster)
print(adjusted_rand_index)
