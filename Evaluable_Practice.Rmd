---
title: "Evaluable Practice"
author: "Pedro Jiménez García-Ligero, Álvaro Luna Ramírez, Nerea Alberdi Arrillaga and Germán José Padua Pleguezuelo"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document:
    toc: yes
    toc_depth: '6'
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 6
    number_sections: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
---
<style>
.math {
  font-size: 8.25pt;options(encoding = 'UTF-8')
}
</style>

<div style="text-align: justify">

© This material is licensed under a **Creative Commons CC BY-NC-ND** attribution which allows *works to be downloaded and shared with others, as long as they are referenced, but may not be modified in any way or used commercially*.


# Index
FALTA

# Introduction


**Loading/installation** of R packages necessary for this practice.

The following source code module is responsible for loading, if they are already installed, all the packages that will be used in this R session.


```{r warning=FALSE, message=FALSE}

#########################################
# Loading necessary packages and reason #
#########################################

# Package required to call 'ggplot' function
library(ggplot2)

# Package required to call 'ggarrange' function
library(ggpubr)

# Package required to call 'scatterplot3d' function
library(scatterplot3d)

# Package required to call 'melt' function
library(reshape2)

# Package required to call 'mvn' function
#library(MVN)

# Package required to call 'boxM' function
library(biotools)

# Package required to call 'partimat' function
library(klaR)

# Package required to call 'summarise' function
library(dplyr)

# Package required to call 'createDataPartition' function
library(caret)

# Package required to call 'freq' and 'descr' functions (descriptive statistics)
library(summarytools)

# Package required to call 'ggplot' function (graphical tools)
library(ggplot2)

# Package required to call 'ggarrange' function (graphical tools)
library(ggpubr)

# Package required to call 'read.spss' function (loading '.spss' data format)
library(foreign)

# Package required to call 'read_xlsx' function (loading '.xlsx' data format)
library(readxl)

# Package required to load the data set 'RBGlass1'
library(archdata)

# Package required to call 'cortest.bartlett' function
library(psych)

# Package required to call 'fviz_pca_var, fviz_pca_ind and fviz_pca' functions
library(factoextra)

# Package required to call 'scatterplot3d' function
library(scatterplot3d)

# Package required to call 'hetcor' function
library(polycor)

# Package required to call 'ggcorrplot' function
library(ggcorrplot)

# Package required to call 'corrplot' function
library(corrplot)

# Package required to call 'rplot' function
library(corrr)

# Package required to call 'mvn' function
library(MVN)

# Package required to plot ROC curve
library(pROC)
library(caret)

# Package required to call 'mutate' function
library(tidyverse)

# Package required to call 'clusGap' function
library(cluster)

# Package required to call 'get_dist', 'fviz_cluster' and 'fviz_dist' functions
library(factoextra)

# Package required to call 'ggdendrogram' function
library(ggdendro)

# Package required to call 'grid.arrange' function
library(gridExtra)

library(mclust)

library(moments)


```


```{r warning=FALSE, message=FALSE}

# Loading the .csv file
# The output of this function is already a data.frame object
data<-read.csv("train.csv", header = TRUE,sep =",", fileEncoding = "UTF-8")

```


# ** Univariate exploratory analysis**

In this phase it is recommended to carry out a preliminary exploratory analysis of the data. To do this, apply the different numerical and graphical techniques used in class. At first, it will focus on the analysis of each variable independently without yet looking for possible interactions between them. It is recommended to perform numerical and graphical analysis of each variable, to detect: data groupings, missing values, classical numeric descriptive analysis, extreme values, assumption of normality. 

```{r warning=FALSE, message=FALSE}

# This line loads the variable names from this data.frame
# So that we can access by their name with no refer to the data.frame identifier
attach(data)

# We will only select 13 variables for our study
data <- data[, c(
  "MSSubClass",
  "MSZoning",
  "LotFrontage",
  "LotArea",
  "OverallQual",
  "OverallCond",
  "YearBuilt",
  "ExterQual",
  "ExterCond",
  "GarageQual",
  "GrLivArea",
  "X1stFlrSF",
  "X2ndFlrSF",
  "YrSold",
  "SaleType",
  "SaleCondition",
  "SalePrice"
)]


# Retrieving the name of the selected variables
colnames(data)

# Displaying a few records
head(data)

```



```{r warning=FALSE, message=FALSE}

# Show the number of NA's values per column
na_counts <- colMeans(is.na(data))

print(na_counts)

```



MSSubClass: Identifies the type of dwelling involved in the sale. (Discrete Numerical)


```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(MSSubClass)
freq(MSSubClass)

# Histogram and density plots
# Remember that package 'ggplot2' is required

p1<-ggplot(data,aes(x=MSSubClass))+geom_histogram()+
  labs(title = "Histogram of MSSubClass",x="MSSubClass",y="Values")


p1

```  


```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$MSSubClass))

print(na_counts)

```    
 No missing values.
 
 
 Classical numeric descriptive analysis
 
```{r warning=FALSE, message=FALSE}

describe(data$MSSubClass)


```
 
 
Outliers

```{r warning=FALSE, message=FALSE}

# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = MSSubClass)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of densidad", x = "Values", y = "")

# Count outliers
outliers <- boxplot.stats(data$MSSubClass)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$MSSubClass %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")



```    
  
There are 103 outliers, as there enough data records, we will eliminate them.


```{r warning=FALSE, message=FALSE}

# Remove rows with outliers
data <- data[-outlier_rows, ]

# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")

``` 


**Assumption of Normality**
It isn't continuous so we skip this step.


MSZoning : Identifies the general zoning classification of the sale.
It is a categorical variable

```{r warning=FALSE, message=FALSE}

# Basic descriptive statistics
# Remember that package 'summarytools' is required
freq(MSZoning)

# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = MSZoning))+geom_bar()+
  coord_polar("y")+labs(x="MSZoning",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = MSZoning))+geom_bar()+
  labs(x="MSZoning",y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)

```

There aren't any missing values, so we will continue with the next variable. 

LotFrontage : Linear feet of street connected to property

```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(LotFrontage)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=LotFrontage))+geom_density()+
  labs(title = "Density of LotFrontage",x="LotFrontage",y="Values")

p2<-ggplot(data,aes(x=LotFrontage))+geom_histogram()+
  labs(title = "Histogram of LotFrontage",x="LotFrontage",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

```  


```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$LotFrontage))

print(na_counts)
print(na_counts/nrow(data))

```    
 248 missing values -> 18.28%
 
 It is a continuous variable, so we will analyze the random pattern with a Student test
 

```{r warning=FALSE, message=FALSE}

# Create a subset for non-missing values
non_missing_values <- data$LotFrontage[!is.na(data$LotFrontage)]

# Perform a t-test for continuous variables
result <- t.test(non_missing_values, data$LotFrontage, alternative = "two.sided")
print(paste("Variable: LotFrontage"))
print(result)


```


    Null Hypothesis (H0): The null hypothesis in a t-test is that there is no significant difference between the means of the two groups.

    Alternative Hypothesis (H1): The alternative hypothesis is that the true difference in means is not equal to 0.

    t-statistic: The t-value is 0. The t-value measures the difference between the means of the two groups relative to the variability in the data. A t-value of 0 suggests that there is no significant difference between the means.

    Degrees of Freedom (df): The degrees of freedom for the Welch Two Sample t-test is 2216.

    p-value: The p-value is 1. This is the probability of observing a t-statistic as extreme as the one computed from the sample, assuming that the null hypothesis is true. A p-value of 1 suggests that there is no evidence to reject the null hypothesis.

    Confidence Interval: The 95 percent confidence interval for the difference in means is from -1.890914 to 1.890914. This interval contains 0, further supporting the idea that there is no significant difference between the means.

    Interpretation: In summary, based on the provided p-value (1), we fail to reject the null hypothesis. There is no significant evidence to suggest that the means of the two groups (missing and non-missing values of LotFrontage) are different. The confidence interval includes 0, reinforcing the lack of a significant difference.


  In the case of homogeneity the pattern is random and, in this case, it is chosen to
replace the NA with the mean


```{r warning=FALSE, message=FALSE}

# Calculate the mean of non-missing values
mean_lot_frontage <- mean(data$LotFrontage, na.rm = TRUE)

# Replace missing values with the mean
data$LotFrontage[is.na(data$LotFrontage)] <- mean_lot_frontage

na_counts <- sum(is.na(data$LotFrontage))

print(na_counts)
print(na_counts/nrow(data))

```


Classical numeric descriptive analysis
 
```{r warning=FALSE, message=FALSE}

describe(data$LotFrontage)

```
```{r warning=FALSE, message=FALSE}

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=data$LotFrontage))+geom_density()+
  labs(title = "Density of LotFrontage",x="LotFrontage",y="Values")

p2<-ggplot(data,aes(x=data$LotFrontage))+geom_histogram()+
  labs(title = "Histogram of LotFrontage",x="LotFrontage",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

```  

Outliers

```{r warning=FALSE, message=FALSE}

# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = LotFrontage)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of LotFrontage", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$LotFrontage)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$LotFrontage %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")

```    
  
There are 47 outliers, as there enough data records, we will eliminate them.

```{r warning=FALSE, message=FALSE}

# Remove rows with outliers
data <- data[-outlier_rows, ]

# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")

```  


**Assumption of Normality**

```{r warning=FALSE, message=FALSE}

# Histogram representation of the column MSSubClass
par(mfcol = c(1, 1))

# Define the column name
j0 <- "LotFrontage"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a histogram for the entire column
hist(data[, j0], prob = TRUE, col = grey(0.8), main = paste("Histogram of", j0), xlab = j0)

# Overlay normal distribution curve
lines(x0, dnorm(x0, mean(data[, j0], na.rm = TRUE), sd(data[, j0], na.rm = TRUE)), col = "blue", lwd = 2)


``` 


**Qqplot graphics**

```{r warning=FALSE, message=FALSE}
# Representation of normal quantiles for the column MSSubClass
par(mfrow = c(1, 1))

# Define the column name
j0 <- "LotFrontage"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a quantile-quantile plot
qqnorm(data[, j0], main = paste("Q-Q Plot of", j0), pch = 19, col = 2)
qqline(data[, j0])


```
  
  
This exploratory analysis can give us an idea of the possible normal distribution of the univariate variables, but it is always better to do the respective normality tests.

**Univariate normality test (Shapiro-Wilk)**

The **null hypothesis** that the data **follow a univariate normal distribution** is tested. This hypothesis is **rejected** if the **p-value** given by the Shapiro-Wilk test is **less than 0.05**. Otherwise the assumption of normality of the data is not rejected.

```{r warning=FALSE, message=FALSE}

# Assuming 'data' is your original data frame
data_tidy <- melt(data, value.name = "value")

# Use the aggregate function with the correct FUN argument
result <- aggregate(value ~ variable, data = data_tidy,
                    FUN = function(x) shapiro.test(x)$p.value)

# Print the row corresponding to LotFrontage
print(subset(result, variable == "LotFrontage"))

```

**There is evidence of lack of univariate normality**  (p-value < 0.05).


LotArea: Lot size in square feet

```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(LotArea)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=LotArea))+geom_density()+
  labs(title = "Density of LotArea",x="LotArea",y="Values")

p2<-ggplot(data,aes(x=LotArea))+geom_histogram()+
  labs(title = "Histogram of LotArea",x="LotArea",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

``` 

```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$LotArea))

print(na_counts)
print(na_counts/nrow(data))

```    
 0 missing values -> 0%
 
 Outliers

```{r warning=FALSE, message=FALSE}
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = LotArea)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of LotArea", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$LotArea)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$LotArea %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")

``` 

There are 59 outliers, as there enough data records, we will eliminate them.

```{r warning=FALSE, message=FALSE}

# Remove rows with outliers
data <- data[-outlier_rows, ]

# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")

```

**Assumption of Normality**

```{r warning=FALSE, message=FALSE}

# Histogram representation of the column LotArea
par(mfcol = c(1, 1))

# Define the column name
j0 <- "LotArea"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a histogram for the entire column
hist(data[, j0], prob = TRUE, col = grey(0.8), main = paste("Histogram of", j0), xlab = j0)

# Overlay normal distribution curve
lines(x0, dnorm(x0, mean(data[, j0], na.rm = TRUE), sd(data[, j0], na.rm = TRUE)), col = "blue", lwd = 2)


``` 

**Qqplot graphics**

```{r warning=FALSE, message=FALSE}
# Representation of normal quantiles for the column LotArea
par(mfrow = c(1, 1))

# Define the column name
j0 <- "LotArea"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a quantile-quantile plot
qqnorm(data[, j0], main = paste("Q-Q Plot of", j0), pch = 19, col = 2)
qqline(data[, j0])


```

This exploratory analysis can give us an idea of the possible normal distribution of the univariate variables, but it is always better to do the respective normality tests.

**Univariate normality test (Shapiro-Wilk)**

The **null hypothesis** that the data **follow a univariate normal distribution** is tested. This hypothesis is **rejected** if the **p-value** given by the Shapiro-Wilk test is **less than 0.05**. Otherwise the assumption of normality of the data is not rejected.

```{r warning=FALSE, message=FALSE}
data_tidy <- melt(data, value.name = "value")

# Use the aggregate function with the correct FUN argument
result <- aggregate(value ~ variable, data = data_tidy,
                    FUN = function(x) shapiro.test(x)$p.value)

# Print the row corresponding to LotArea
print(subset(result, variable == "LotArea"))

```

**There is evidence of lack of univariate normality**  (p-value < 0.05).


OverallQual: Rates the overall material and finish of the house (1 - Very Poor , 10 - Very Excellent) (Ordinal Numerical Discrete)

```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(OverallQual)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=OverallQual))+geom_density()+
  labs(title = "Density of OverallQual",x="OverallQual",y="Values")

p2<-ggplot(data,aes(x=OverallQual))+geom_histogram()+
  labs(title = "Histogram of OverallQual",x="OverallQual",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

```  

```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$OverallQual))

print(na_counts)

``` 

No missing values.
 
 
Outliers

```{r warning=FALSE, message=FALSE}

# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = OverallQual)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of OverallQual", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$OverallQual)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$OverallQual %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")



```    
  
There are 2 outliers, as there enough data records, we will eliminate them.

```{r warning=FALSE, message=FALSE}

# Remove rows with outliers
data <- data[-outlier_rows, ]

# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")

``` 


**Assumption of Normality**
It isn't a continuous variable so we skip this step.

  
  

OverallCond: Rates the overall condition of the house (1 - Very Poor , 10 - Very Excellent)


```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(OverallCond)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=OverallCond))+geom_density()+
  labs(title = "Density of OverallCond",x="OverallCond",y="Values")

p2<-ggplot(data,aes(x=OverallCond))+geom_histogram()+
  labs(title = "Histogram of OverallCond",x="OverallCond",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

```  

```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$OverallCond))

print(na_counts)

``` 

No missing values.

Outliers

```{r warning=FALSE, message=FALSE}

# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = OverallCond)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of OverallCond", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$OverallCond)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$OverallCond %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")



```    
  
There are 106 outliers, as there enough data records, we will eliminate them.

```{r warning=FALSE, message=FALSE}

# Remove rows with outliers
data <- data[-outlier_rows, ]

# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")

``` 


**Assumption of Normality**
It isn't a continuous variable so we skip this step.


YearBuilt: Original construction date

```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(YearBuilt)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=YearBuilt))+geom_density()+
  labs(title = "Density of YearBuilt",x="YearBuilt",y="Values")

p2<-ggplot(data,aes(x=YearBuilt))+geom_histogram()+
  labs(title = "Histogram of YearBuilt",x="YearBuilt",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

``` 

```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$YearBuilt))

print(na_counts)
print(na_counts/nrow(data))

```    
 0 missing values 
 
 Outliers

```{r warning=FALSE, message=FALSE}
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = YearBuilt)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of YearBuilt", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$YearBuilt)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$YearBuilt %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")

``` 

There are 5 outliers, as there enough data records, we will eliminate them.

```{r warning=FALSE, message=FALSE}

# Remove rows with outliers
data <- data[-outlier_rows, ]

# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")

```

**Assumption of Normality**
It isn't a continuous variable so we skip this step.



ExterQual: Evaluates the quality of the material on the exterior

```{r warning=FALSE, message=FALSE}

# Basic descriptive statistics
# Remember that package 'summarytools' is required
freq(ExterQual)

# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = ExterQual))+geom_bar()+
  coord_polar("y")+labs(x="ExterQual",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = ExterQual))+geom_bar()+
  labs(x="ExterQual",y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)

``` 
There aren't any missing values, so we will continue with the next variable. 



ExterCond: Evaluates the present condition of the material on the exterior
```{r warning=FALSE, message=FALSE}

# Basic descriptive statistics
# Remember that package 'summarytools' is required
freq(ExterCond)

# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = ExterCond))+geom_bar()+
  coord_polar("y")+labs(x="ExterCond",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = ExterCond))+geom_bar()+
  labs(x="ExterCond",y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)

``` 
There aren't any missing values, so we will continue with the next variable. 

  
GarageQual: Garage quality
```{r warning=FALSE, message=FALSE}

# Basic descriptive statistics
# Remember that package 'summarytools' is required
freq(GarageQual)

# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = GarageQual))+geom_bar()+
  coord_polar("y")+labs(x="GarageQual",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = GarageQual))+geom_bar()+
  labs(x="GarageQual",y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)

``` 
NA values in this variable mean that there isn't a garage, so we don't have to impute those values. 


GrLivArea: Above grade (ground) living area square feet
```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(GrLivArea)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=GrLivArea))+geom_density()+
  labs(title = "Density of GrLivArea",x="GrLivArea",y="Values")

p2<-ggplot(data,aes(x=YearBuilt))+geom_histogram()+
  labs(title = "Histogram of GrLivArea",x="GrLivArea",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

``` 

```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$YearBuilt))

print(na_counts)
print(na_counts/nrow(data))

```  

0 missing values 
 
 Outliers

```{r warning=FALSE, message=FALSE}
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = GrLivArea)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of GrLivArea", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$GrLivArea)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$GrLivArea %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")

``` 
There are 17 outliers, as there enough data records, we will eliminate them.

```{r warning=FALSE, message=FALSE}

# Remove rows with outliers
data <- data[-outlier_rows, ]

# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")

```

**Assumption of Normality**

```{r warning=FALSE, message=FALSE}

# Histogram representation of the column GrLivArea
par(mfcol = c(1, 1))

# Define the column name
j0 <- "GrLivArea"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a histogram for the entire column
hist(data[, j0], prob = TRUE, col = grey(0.8), main = paste("Histogram of", j0), xlab = j0)

# Overlay normal distribution curve
lines(x0, dnorm(x0, mean(data[, j0], na.rm = TRUE), sd(data[, j0], na.rm = TRUE)), col = "blue", lwd = 2)


``` 

**Qqplot graphics**

```{r warning=FALSE, message=FALSE}
# Representation of normal quantiles for the column GrLivArea
par(mfrow = c(1, 1))

# Define the column name
j0 <- "GrLivArea"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a quantile-quantile plot
qqnorm(data[, j0], main = paste("Q-Q Plot of", j0), pch = 19, col = 2)
qqline(data[, j0])


```

This exploratory analysis can give us an idea of the possible normal distribution of the univariate variables, but it is always better to do the respective normality tests.

**Univariate normality test (Shapiro-Wilk)**

The **null hypothesis** that the data **follow a univariate normal distribution** is tested. This hypothesis is **rejected** if the **p-value** given by the Shapiro-Wilk test is **less than 0.05**. Otherwise the assumption of normality of the data is not rejected.

```{r warning=FALSE, message=FALSE}

# Assuming 'data' is your original data frame
data_tidy <- melt(data, value.name = "value")

# Use the aggregate function with the correct FUN argument
result <- aggregate(value ~ variable, data = data_tidy,
                    FUN = function(x) shapiro.test(x)$p.value)

# Print the row corresponding to YearBuilt
print(subset(result, variable == "YearBuilt"))

```

**There is evidence of lack of univariate normality**  (p-value < 0.05).


1stFlrSF: First Floor square feet 

```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(X1stFlrSF)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=X1stFlrSF))+geom_density()+
  labs(title = "Density of 1stFlrSF",x="1stFlrSF",y="Values")

p2<-ggplot(data,aes(x=X1stFlrSF))+geom_histogram()+
  labs(title = "Histogram of 1stFlrSF",x="1stFlrSF",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

``` 

```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$X1stFlrSF))

print(na_counts)
print(na_counts/nrow(data))

```  

0 missing values 
 
 Outliers

```{r warning=FALSE, message=FALSE}
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = X1stFlrSF)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of X1stFlrSF", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$X1stFlrSF)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$X1stFlrSF %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")

``` 

There are 13 outliers, as there enough data records, we will eliminate them.

```{r warning=FALSE, message=FALSE}

# Remove rows with outliers
data <- data[-outlier_rows, ]

# Print a message after removing outliers
cat("Outliers removed. New number of rows:", nrow(data), "\n")

```

**Assumption of Normality**

```{r warning=FALSE, message=FALSE}

# Histogram representation of the column X1stFlrSF
par(mfcol = c(1, 1))

# Define the column name
j0 <- "X1stFlrSF"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a histogram for the entire column
hist(data[, j0], prob = TRUE, col = grey(0.8), main = paste("Histogram of", j0), xlab = j0)

# Overlay normal distribution curve
lines(x0, dnorm(x0, mean(data[, j0], na.rm = TRUE), sd(data[, j0], na.rm = TRUE)), col = "blue", lwd = 2)


``` 

**Qqplot graphics**

```{r warning=FALSE, message=FALSE}
# Representation of normal quantiles for the column X1stFlrSF
par(mfrow = c(1, 1))

# Define the column name
j0 <- "X1stFlrSF"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a quantile-quantile plot
qqnorm(data[, j0], main = paste("Q-Q Plot of", j0), pch = 19, col = 2)
qqline(data[, j0])


```

This exploratory analysis can give us an idea of the possible normal distribution of the univariate variables, but it is always better to do the respective normality tests.

**Univariate normality test (Shapiro-Wilk)**

The **null hypothesis** that the data **follow a univariate normal distribution** is tested. This hypothesis is **rejected** if the **p-value** given by the Shapiro-Wilk test is **less than 0.05**. Otherwise the assumption of normality of the data is not rejected.

```{r warning=FALSE, message=FALSE}

# Assuming 'data' is your original data frame
data_tidy <- melt(data, value.name = "value")

# Use the aggregate function with the correct FUN argument
result <- aggregate(value ~ variable, data = data_tidy,
                    FUN = function(x) shapiro.test(x)$p.value)

# Print the row corresponding to X1stFlrSF
print(subset(result, variable == "X1stFlrSF"))

```

**There is evidence of lack of univariate normality**  (p-value < 0.05).




X2ndFlrSF: Second floor square feet

```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(X2ndFlrSF)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=X2ndFlrSF))+geom_density()+
  labs(title = "Density of X2ndFlrSF",x="X2ndFlrSF",y="Values")

p2<-ggplot(data,aes(x=X2ndFlrSF))+geom_histogram()+
  labs(title = "Histogram of X2ndFlrSF",x="X2ndFlrSF",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

``` 

```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$X2ndFlrSF))

print(na_counts)
print(na_counts/nrow(data))

```  

0 missing values 
 
 Outliers

```{r warning=FALSE, message=FALSE}
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = X2ndFlrSF)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of X2ndFlrSF", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$X2ndFlrSF)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$X2ndFlrSF %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")

``` 

There are 0 outliers.

**Assumption of Normality**

```{r warning=FALSE, message=FALSE}

# Histogram representation of the column X1stFlrSF
par(mfcol = c(1, 1))

# Define the column name
j0 <- "X2ndFlrSF"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a histogram for the entire column
hist(data[, j0], prob = TRUE, col = grey(0.8), main = paste("Histogram of", j0), xlab = j0)

# Overlay normal distribution curve
lines(x0, dnorm(x0, mean(data[, j0], na.rm = TRUE), sd(data[, j0], na.rm = TRUE)), col = "blue", lwd = 2)


``` 

**Qqplot graphics**

```{r warning=FALSE, message=FALSE}
# Representation of normal quantiles for the column X2ndFlrSF
par(mfrow = c(1, 1))

# Define the column name
j0 <- "X2ndFlrSF"

# Set the sequence for the x-axis
x0 <- seq(min(data[, j0], na.rm = TRUE), max(data[, j0], na.rm = TRUE), length.out = 50)

# Create a quantile-quantile plot
qqnorm(data[, j0], main = paste("Q-Q Plot of", j0), pch = 19, col = 2)
qqline(data[, j0])


```

This exploratory analysis can give us an idea of the possible normal distribution of the univariate variables, but it is always better to do the respective normality tests.

**Univariate normality test (Shapiro-Wilk)**

The **null hypothesis** that the data **follow a univariate normal distribution** is tested. This hypothesis is **rejected** if the **p-value** given by the Shapiro-Wilk test is **less than 0.05**. Otherwise the assumption of normality of the data is not rejected.

```{r warning=FALSE, message=FALSE}

# Assuming 'data' is your original data frame
data_tidy <- melt(data, value.name = "value")

# Use the aggregate function with the correct FUN argument
result <- aggregate(value ~ variable, data = data_tidy,
                    FUN = function(x) shapiro.test(x)$p.value)

# Print the row corresponding to X2ndFlrSF
print(subset(result, variable == "X2ndFlrSF"))

```

**There is evidence of lack of univariate normality**  (p-value < 0.05).





YrSold: Year Sold (YYYY)

```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(YrSold)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=YrSold))+geom_density()+
  labs(title = "Density of YrSold",x="YrSold",y="Values")

p2<-ggplot(data,aes(x=YrSold))+geom_histogram()+
  labs(title = "Histogram of YrSold",x="YrSold",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

``` 

```{r warning=FALSE, message=FALSE}
# Missing Values

na_counts <- sum(is.na(data$YrSold))

print(na_counts)
print(na_counts/nrow(data))

```  

0 missing values 
 
 Outliers

```{r warning=FALSE, message=FALSE}
# Boxplot to visualize outliers
p1 <- ggplot(data, aes(x = YrSold)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 1, outlier.size = 2) +
  coord_flip() +
  labs(title = "Boxplot of YrSold", x = "Values", y = "")

ggarrange(p1, nrow=1, common.legend = FALSE)

# Count outliers
outliers <- boxplot.stats(data$YrSold)$out

# Display the count of outliers
cat("Number of outliers:", length(outliers), "\n")

# Identify rows with outliers
outlier_rows <- which(data$YrSold %in% outliers)

# Print the indices of rows with outliers
cat("Indices of rows with outliers:", outlier_rows, "\n")

``` 

There are 0 outliers.

**Assumption of Normality**
It isn't a continuous variable so we skip this step.



SaleType: Type of sale

```{r warning=FALSE, message=FALSE}

# Basic descriptive statistics
# Remember that package 'summarytools' is required
freq(SaleType)

# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = SaleType))+geom_bar()+
  coord_polar("y")+labs(x="SaleType",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = SaleType))+geom_bar()+
  labs(x="SaleType",y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)

``` 
There aren't any missing values, so we will continue with the next variable. 



SaleCondition: Condition of sale

```{r warning=FALSE, message=FALSE}

# Basic descriptive statistics
# Remember that package 'summarytools' is required
freq(SaleCondition)

# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = SaleCondition))+geom_bar()+
  coord_polar("y")+labs(x="SaleCondition",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = SaleCondition))+geom_bar()+
  labs(x="SaleCondition",y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)

``` 
There aren't any missing values, so we will continue with the next variable.




SalePrice
```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(SalePrice)

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=SalePrice))+geom_density()+
  labs(title = "Density of SalePrice",x="SalePrice",y="Values")

p2<-ggplot(data,aes(x=SalePrice))+geom_histogram()+
  labs(title = "Histogram of SalePrice",x="SalePrice",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

``` 

We can perform a log transformation to correct the skewness of the variable

```{r warning=FALSE, message=FALSE}
# Classical numeric descriptive analysis
describe(log(SalePrice))

# Histogram and density plots
# Remember that package 'ggplot2' is required
p1<-ggplot(data,aes(x=log(SalePrice)))+geom_density()+
  labs(title = "Density of SalePrice",x="SalePrice",y="Values")

p2<-ggplot(data,aes(x=log(SalePrice)))+geom_histogram()+
  labs(title = "Histogram of SalePrice",x="SalePrice",y="Values")


# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2, nrow=1, common.legend = FALSE)

``` 


We will split this target variable into two categories (intervals) of equal amplitude.  

```{r warning=FALSE, message=FALSE}

# Dividir SalePrice en dos grupos de igual frecuencia
data$PriceCategory <- ntile(data$SalePrice, 2)

# Asignar nombres a las categorías
data$PriceCategory <- ifelse(data$PriceCategory == 1, "Cheap", "Expensive")

# Resto del código para análisis descriptivo y gráficos
describe(data$SalePrice)

p1 <- ggplot(data, aes(x = SalePrice)) + 
      geom_density() + 
      labs(title = "Density of SalePrice", x = "SalePrice", y = "Density")

p2 <- ggplot(data, aes(x = SalePrice)) + 
      geom_histogram(bins = 30) + 
      labs(title = "Histogram of SalePrice", x = "SalePrice", y = "Count")

ggarrange(p1, p2, nrow = 1, common.legend = FALSE)

# Verificar la nueva variable
table(data$PriceCategory)


``` 

```{r warning=FALSE, message=FALSE}
# Drop the SalePrice variable
data <- select(data, -SalePrice)

# To verify that the column has been removed, you can view the structure of the dataframe
str(data)
```

```{r warning=FALSE, message=FALSE}
# Basic descriptive statistics
# Remember that package 'summarytools' is required
freq(data$PriceCategory)

# Pie chart and bar graph
p1<-ggplot(data, aes(x=factor(1), fill = PriceCategory))+geom_bar()+
  coord_polar("y")+labs(x="SaleType",y="%")
p2<-ggplot(data, aes(x=factor(1), fill = PriceCategory))+geom_bar()+
  labs(x="PriceCategory",y="%")

# This function controls the graphical output device
# Remember that package 'ggpubr' is required
ggarrange(p1,p2,nrow = 1,ncol=2, common.legend = TRUE)

``` 

There aren't any missing values.

We have finished our univariate exploratory analysis. We continue with the Multivariate exploratory analysis.

# ** Multivariate exploratory analysis**


First, we check the assumptions of correlation between variables with the Bartlett test.



### Correlated variables

According to the numerical results below, it is observed that the data **are correlated** both **at the sample level** (see correlation matrix) and **at the populacion level** (Bartlett's sphericity test is significant).

```{r warning=FALSE, message=FALSE}

###############################
# Correlation at sample level #
###############################

# Select only numeric columns
numeric_data <- data[sapply(data, is.numeric)]

# Are the variables correlated at sample level?
correlation_matrix <- cor(numeric_data)

# Display the first 6x6 portion of the correlation matrix
correlation_matrix

# Compute the determinant of the correlation matrix (for full matrix)
det(correlation_matrix)


###################################
# Correlation at population level #
###################################

# Bartlett's sphericity test:
# This test checks whether the correlations are significantly different from 0
# The null hypothesis is H_0; det(R)=1 means the variables are uncorrelated 
# R denotes the correlation matrix
# cortest.bartlett function in the package pysch performs this test
# This function works with standardized data.

# Standardization
numeric_data_scale<-scale(numeric_data)

# Bartlett's sphericity test
cortest.bartlett(cor(numeric_data_scale))

```


Different graphical outputs are illustrated below that provide an intuitive idea of the correlation between the variables. A *trained eye* could anticipate the appropriate number of factors with this visual information.

```{r warning=FALSE, message=FALSE}

# Polychoric correlation matrix
# Remember that package 'polycor' is required for 'hetcor' function
poly_cor<-hetcor(numeric_data)$correlations

# Remember that package 'ggcorrplot' is required for 'ggcorrplot' function
ggcorrplot(poly_cor, type="lower",hc.order=T)

# Another interesting visual representation is the following
# Remember that package 'corrplot' is required for 'corrplot' function
corrplot(cor(numeric_data), order = "hclust", tl.col='black', tl.cex=1)



```

### Absence of outliers and missing values

Done in previous sections.


### Principal Component Analysis

#### Obtaining


```{r warning=FALSE, message=FALSE}

# The 'prcomp' function in the base R package performs this analysis
# Parameters 'scale' and 'center' are set to TRUE to consider standardized data
PCA<-prcomp(numeric_data, scale=T, center = T)

# The field 'rotation' of the 'PCA' object is a matrix 
# Its columns are the coefficients of the principal components
# Indicates the weight of each variable in the corresponding principal component
PCA$rotation

# Standard deviations of each principal component
PCA$sdev
```


Each principal component is obtained in a simple way as a linear combination of all the variables with the coefficients indicated by the columns of the rotation matrix.


#### Explained variance rate

```{r warning=FALSE, message=FALSE}

# The function 'summary' applied to the 'PCA' object provides relevant information
# - Standard deviations of each principal component
# - Proportion of variance explained and cummulative variance
summary(PCA)

# The following graph shows the proportion of explained variance
Explained_variance <- PCA$sdev^2 / sum(PCA$sdev^2)

p1<-ggplot(data = data.frame(Explained_variance, pc = 1:10),
  aes(x = pc, y = Explained_variance, fill=Explained_variance )) +
  geom_col(width = 0.3) + scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Principal component", y= "Proportion of variance")

# The following graph shows the proportion of cumulative explained variance
Cummulative_variance<-cumsum(Explained_variance)

p2<-ggplot( data = data.frame(Cummulative_variance, pc = 1:10),
  aes(x = pc, y = Cummulative_variance ,fill=Cummulative_variance )) +
  geom_col(width = 0.5) +  scale_y_continuous(limits = c(0,1)) + theme_bw() +
  labs(x = "Principal component",
       y = "Proportion of cumulative variance")

p1
p2

fviz_eig(PCA)
```


#### Appropriate number of principal components

There are different methods:

+ 1.- **Elbow method** (Cuadras, 2007).
+ 2.- **At the discretion of the researcher** who chooses a minimum percentage of variance explained by the principal components (it is not reliable because it can give more than necessary).
+ 3.- **Rule of Abdi et al.** (2010). The variances explained by the principal components are averaged and those whose proportion of explained variance exceeds the mean are selected.

For this illustration, applying the rule of Abdi et al., only **four principal components are considered**, as can be deduced from the following code chunk.

```{r warning=FALSE, message=FALSE}

#######################
# Rule of Abdi et al. #
#######################

# Variances
PCA$sdev^2

# Average of variances
mean(PCA$sdev^2)

```



#### PCA graphical outputs of interest 

##### Distances 


```{r warning=FALSE, message=FALSE}

# These graphical outputs show the projection of the variables in two dimensions
# Display the weight of the variable in the direction of the principal component 

# Projection of variables on the first and second principal components
p1 <- fviz_pca_var(PCA, axes = c(1, 2), repel = TRUE, col.var = "cos2",
                   legend.title = "Distance", title = "Variables (PC1 vs PC2)") + theme_bw()

# Projection on the first and third principal components
p2 <- fviz_pca_var(PCA, axes = c(1, 3), repel = TRUE, col.var = "cos2",
                   legend.title = "Distance", title = "Variables (PC1 vs PC3)") + theme_bw()

# Projection on the first and fourth principal components
p3 <- fviz_pca_var(PCA, axes = c(1, 4), repel = TRUE, col.var = "cos2",
                   legend.title = "Distance", title = "Variables (PC1 vs PC4)") + theme_bw()

# Projection on the second and third principal components
p4 <- fviz_pca_var(PCA, axes = c(2, 3), repel = TRUE, col.var = "cos2",
                   legend.title = "Distance", title = "Variables (PC2 vs PC3)") + theme_bw()

# Projection on the second and fourth principal components
p5 <- fviz_pca_var(PCA, axes = c(2, 4), repel = TRUE, col.var = "cos2",
                   legend.title = "Distance", title = "Variables (PC2 vs PC4)") + theme_bw()

# Projection on the third and fourth principal components
p6 <- fviz_pca_var(PCA, axes = c(3, 4), repel = TRUE, col.var = "cos2",
                   legend.title = "Distance", title = "Variables (PC3 vs PC4)") + theme_bw()

# Displaying graphics
p1
p2
p3
p4
p5
p6

```



##### Observations and variance contribution


```{r warning=FALSE, message=FALSE}

# It is also possible to represent the observations
# As well as identify with colors those observations that explain the greatest 
# variance of the principal components
p1<-fviz_pca_ind(PCA,axes = c(1,2),col.ind = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()


p2<-fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()

p3<-fviz_pca_ind(PCA,axes=c(1,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()

p4<-fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()

p5<-fviz_pca_ind(PCA,axes=c(2,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()

p6<-fviz_pca_ind(PCA,axes=c(3,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var", title="Records")+theme_bw()

# Displaying graphics
p1
p2
p3
p4
p5
p6

```



##### Observations and variables with variance contribution



```{r warning=FALSE, message=FALSE}

# Joint representation of variables and observations
# Relates the possible relationships between the contributions of the records
# to the variances of the components and the weight of the variables in each 
# principal component

p1<-fviz_pca(PCA,axes=c(1,2),alpha.ind ="contrib", col.var = "cos2",
         col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

p2<-fviz_pca(PCA,axes=c(1,3),alpha.ind ="contrib", 
         col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

p3<-fviz_pca(PCA,axes=c(1,4),alpha.ind ="contrib", 
         col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

p4<-fviz_pca(PCA,axes=c(2,3),alpha.ind ="contrib", 
         col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

p5<-fviz_pca(PCA,axes=c(2,4),alpha.ind ="contrib", 
         col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

p6<-fviz_pca(PCA,axes=c(3,4),alpha.ind ="contrib", 
         col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE, legend.title="Distancia")+theme_bw()

# Displaying graphics
p1
p2
p3
p4
p5
p6

```


##### Coordinates in the new reference system


Finally, since the object of this part was to reduce the dimension of the data set, it is possible to obtain **the coordinates of the original data in the new reference system**. In fact, they are stored since we used the prcomp function to create the PCA variable.

```{r warning=FALSE, message=FALSE}

head(PCA$x)

```


### Factorial Analysis

#### Appropriate number of latent factors

There are different criteria, among which the **Scree plot** (Cattel 1966) and **parallel analysis** (Horn 1965) stand out. According to the following graphical outputs, **5** is considered to be the **optimal number of factors** (parallel analysis), despite **3** are the appropriate according to the first graphical scree plot.

```{r warning=FALSE, message=FALSE}

# Scree plot
scree(poly_cor)

#Parallel analysis
fa.parallel(poly_cor,n.obs=100,fa="fa",fm="ml")


```


The factorial model with 3 factors implementing a varimax rotation to seek a simpler interpretation is performed.

```{r warning=FALSE, message=FALSE}

modelo_varimax<-fa(poly_cor,nfactors = 3,rotate = "varimax",
                   fa="mle")

# The rotated factorial matrix is shown
print(modelo_varimax$loadings,cut=0) 

```

Visually we could make the effort to see what variables each correlates with one of the factors, but it is very tedious. So we use the following representation in diagram mode.

In this diagram, among other things, it is seen that the first factor is associated with the items E1, E2, E3, E4, E5 and N4, which are the items of the questionnaire that try to identify the quality of extraversion.

```{r warning=FALSE, message=FALSE}

fa.diagram(modelo_varimax)

```



Another way to do the previous analysis.

```{r warning=FALSE, message=FALSE}

# Remember that package 'stats' is required for 'factanal' function
# This function only performs the mle method
FA<-factanal(numeric_data,factors=3, rotation="varimax")
FA$loadings


```




### Multivariate normality


```{r warning=FALSE, message=FALSE}


# Royston multivariate normality test
royston_test <- mvn(data = numeric_data, mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality

# Henze-Zirkler multivariate normality test
hz_test <- mvn(data = numeric_data, mvnTest = "hz")
hz_test$multivariateNormality

```

We find evidence of a lack of multivariate normality at the 5% level.




### Clasifier using linear discriminant analysis

In this example, **we will work with a partition of the dataset, as a training set,** on which we will perform the quadratic discriminant analysis, and **another partition, as a test set,** with which we perform the validation of the model.


```{r warning=FALSE, message=FALSE}

# The response variable has to be an object of the class 'factor' of R language
data$PriceCategory<-as.factor(data$PriceCategory)


set.seed(3)

# Partitioning the dataset: training (80%) + test (20%)
trainIndex<-createDataPartition(data$PriceCategory,p=0.80)$Resample
datos_train<-data[trainIndex,]
datos_test<-data[-trainIndex,]

datos_train
datos_test
```


The **lda** function of the **MASS** package adjusts this model.

```{r warning=FALSE, message=FALSE}
modelo_lda <- lda(formula = PriceCategory ~ LotFrontage + LotArea + OverallQual + OverallCond + YearBuilt + GrLivArea + X1stFlrSF + X2ndFlrSF + YrSold ,data = datos_train)
modelo_lda

```

```{r warning=FALSE, message=FALSE}

nuevas_observaciones <- datos_test
predict(object = modelo_lda, newdata = nuevas_observaciones)

```



The **confusionmatrix** function of the **biotools** package performs cross-validation of the classification model.

```{r warning=FALSE, message=FALSE}

pred <- predict(object = modelo_lda, newdata = datos_test)
confusionmatrix(datos_test$PriceCategory, pred$class)

# Classification error percentage
trainig_error <- mean(datos_test$PriceCategory != pred$class) * 100
paste("trainig_error=", trainig_error, "%")

```


The **correct classifications rate is ???%**

From a geometric point of view, linear discriminant analysis separates space using a straight line. 

#### ROC curve
```{r warning=FALSE, message=FALSE}

# Asegúrate de obtener las probabilidades predichas
pred <- predict(object = modelo_lda, newdata = datos_test, type = "response")

# Calcula la curva ROC usando las probabilidades y la clase verdadera
roc_curve <- roc(datos_test$PriceCategory, as.numeric(pred$posterior[,2]), levels = c("Cheap", "Expensive"))

# Dibuja la curva ROC
plot(roc_curve, main="Curva ROC para LDA", col="#1c61b6")

# Agrega línea diagonal (clasificador aleatorio)
abline(0, 1, lty=2, col = "red")

# Opcional: Calcula el área bajo la curva (AUC)
auc(roc_curve)

```


#### Validity measures

```{r warning=FALSE, message=FALSE}

# Calcula la matriz de confusión
conf_mat <- confusionMatrix(pred$class, datos_test$PriceCategory)

# Extrae los valores de TP, FP, TN, FN
TP <- conf_mat$table[2,2]
FP <- conf_mat$table[1,2]
TN <- conf_mat$table[1,1]
FN <- conf_mat$table[2,1]

# Calcula las métricas
PPV <- TP / (TP + FP)  # Positive Predictive Value
TPR <- TP / (TP + FN)  # True Positive Rate o Sensitivity
TNR <- TN / (TN + FP)  # True Negative Rate o Specificity
F1 <- 2 * PPV * TPR / (PPV + TPR)  # F1 Score
G_mean <- sqrt(TPR * TNR)  # G-mean
Accuracy <- (TP + TN) / (TP + FP + FN + TN)  # Accuracy

# Calcula AUC
roc_curve <- roc(datos_test$PriceCategory, as.numeric(pred$posterior[,2]), levels = c("Cheap", "Expensive"))
AUC <- auc(roc_curve)

# Imprime las métricas
cat("PPV:", PPV, "\n",
    "TPR:", TPR, "\n",
    "TNR:", TNR, "\n",
    "F1-score:", F1, "\n",
    "G-mean:", G_mean, "\n",
    "Accuracy:", Accuracy, "\n",
    "AUC:", AUC, "\n")

```



### Classifier using quadratic discriminant analysis



Although the assumption of multivariate normality is not verified, taking into account that the variances are not homogeneous, **a quadratic discriminant model is adjusted** because it is robust against the lack of this assumption, although it must be kept in mind given the possibility of obtaining unexpected results.

The **qda** function of the **MASS** package performs the sorting.

```{r warning=FALSE, message=FALSE}

modelo_qda <- qda(PriceCategory ~ LotFrontage + LotArea + OverallQual + OverallCond + YearBuilt + GrLivArea + X1stFlrSF + X2ndFlrSF + YrSold ,data = datos_train)
modelo_qda

```

The output of this object shows us the **prior probabilities** of each group, in this case 0.5 and the **means of each regressor per group**.

Once the classifier is built, we can classify new data based on its measurements by simply calling the **predict** function. For example, **we are going to classify all the observations in the test dataset**.

```{r warning=FALSE, message=FALSE}

nuevas_observaciones <- datos_test
predict(object = modelo_qda, newdata = nuevas_observaciones)

```



The **confusionmatrix** function of the **biotools** package performs cross-validation of the classification model.

```{r warning=FALSE, message=FALSE}

pred <- predict(object = modelo_qda, newdata = datos_test)
confusionmatrix(datos_test$PriceCategory, pred$class)

# Classification error percentage
trainig_error <- mean(datos_test$PriceCategory != pred$class) * 100
paste("trainig_error=", trainig_error, "%")

```

In this case the **correct classifications rate** is ???%.

#### ROC curve

```{r warning=FALSE, message=FALSE}

# Asegúrate de obtener las probabilidades predichas
pred <- predict(object = modelo_qda, newdata = datos_test, type = "response")

# Calcula la curva ROC usando las probabilidades y la clase verdadera
roc_curve <- roc(datos_test$PriceCategory, as.numeric(pred$posterior[,2]), levels = c("Cheap", "Expensive"))

# Dibuja la curva ROC
plot(roc_curve, main="Curva ROC para LDA", col="#1c61b6")

# Agrega línea diagonal (clasificador aleatorio)
abline(0, 1, lty=2, col = "red")

# Opcional: Calcula el área bajo la curva (AUC)
auc(roc_curve)

```


#### Validity measures

```{r warning=FALSE, message=FALSE}

# Calcula la matriz de confusión
conf_mat <- confusionMatrix(pred$class, datos_test$PriceCategory)

# Extrae los valores de TP, FP, TN, FN
TP <- conf_mat$table[2,2]
FP <- conf_mat$table[1,2]
TN <- conf_mat$table[1,1]
FN <- conf_mat$table[2,1]

# Calcula las métricas
PPV <- TP / (TP + FP)  # Positive Predictive Value
TPR <- TP / (TP + FN)  # True Positive Rate o Sensitivity
TNR <- TN / (TN + FP)  # True Negative Rate o Specificity
F1 <- 2 * PPV * TPR / (PPV + TPR)  # F1 Score
G_mean <- sqrt(TPR * TNR)  # G-mean
Accuracy <- (TP + TN) / (TP + FP + FN + TN)  # Accuracy

# Calcula AUC
roc_curve <- roc(datos_test$PriceCategory, as.numeric(pred$posterior[,2]), levels = c("Cheap", "Expensive"))
AUC <- auc(roc_curve)

# Imprime las métricas
cat("PPV:", PPV, "\n",
    "TPR:", TPR, "\n",
    "TNR:", TNR, "\n",
    "F1-score:", F1, "\n",
    "G-mean:", G_mean, "\n",
    "Accuracy:", Accuracy, "\n",
    "AUC:", AUC, "\n")

```


### Cluster Analysis

```{r warning=FALSE, message=FALSE}

distance<- get_dist(numeric_data)
fviz_dist(distance, gradient = list(low ="yellow", mid = "white", high = "brown"))

```


## **Hierarchical clustering**: Ward's method

**Hierarchical clustering** is interested in finding a hierarchy based on the closeness or similarity of the data according to the distance considered. In the **agglomerative** case, we start from a group with the closest observations. The next closest pairs are then calculated and groups are generated in an ascending manner. This construction can be observed visually by means of a **dendrogram**.

Below it will be illustrated how the groups are defined by the number of vertical lines in the dendrogram, and the selection of the optimal number of groups can be estimated from this same graph.

```{r warning=FALSE, message=FALSE}

dendrogram <- hclust(dist(numeric_data, method = 'euclidean'), method = 'ward.D')
ggdendrogram(dendrogram, rotate = FALSE, labels = FALSE, theme_dendro = TRUE) + 
  labs(title = "Dendrograma")

```



## **Non-hierarchical clustering**: K-means algorithm

Click this [link](https://uc-r.github.io/kmeans_clustering) to see a detailed description of this algorithm.

The R language implements the **K-means algorithm** with the function of the same name. This function receives as input parameters the data and the number of groupings to be performed (*centers* parameter). To address the problem of **choosing initial seed points** it incorporates the *nstart* parameter that tests multiple initial configurations and reports on the best one. For example, if *nstart = 25*, it will generate 25 initial configurations. The use of this parameter is recommended.

For this first example the *kmeans* function builds two clusters.

```{r warning=FALSE, message=FALSE}

k2 <- kmeans(numeric_data, centers = 2, nstart = 25)

# Displaying all the fields of the object k2
str(k2)

```

The output provided by the *kmeans* function is a list of information, including the following:

* *cluster*: is a vector of integers, from 1 to K (K=2 in this case), which indicates the cluster in which each observation has been located.
* *centers*: a matrix with the successive centers of the clusters.
* *totss*: the total sum of squares.
* *withinss*: sum of squares vector within each cluster (one component per cluster).
* *tot.withinss*: total sum of squares of the clusters, i.e. sum(withinss).
* *betweens*: sum of squares between groups, i.e. totss-tot.withinss.
* *size*: the number of observations in each cluster.


When displaying the variable *k2* it is seen how the groupings result in 2 grouping sizes of 30 and 20 states. The centers of each group (means) in the four variables (Murder, Assault, UrbanPop, Rape) are also seen. And finally the group assignment for each observation (i.e., Alabama was assigned to group 2, Arkansas was assigned to group 1, etc.).

```{r warning=FALSE, message=FALSE}
k2

```


```{r warning=FALSE, message=FALSE}

fviz_cluster(k2,data=numeric_data)

```


```{r}

set.seed(123)
fviz_nbclust(numeric_data, kmeans, method = "wss")

```


```{r}

set.seed(123)
fviz_nbclust(df, kmeans, method = "silhouette")

```


```{r}

set.seed(123)
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

```

```{r}



```

```{r warning=FALSE, message=FALSE}

# Agregar las etiquetas de cluster al conjunto de datos original (asegúrate de que las filas coincidan)
data$Cluster <- k2$cluster

# Crear una tabla cruzada para comparar los clusters con PriceCategory
table(data$PriceCategory, data$Cluster)

```

It coincides in the majority of the examples.


```{r warning=FALSE, message=FALSE}

adjusted_rand_index <- adjustedRandIndex(data$PriceCategory, data$Cluster)
print(adjusted_rand_index)

```
´
 El valor del ARI es 0.06957727. El ARI es una medida de la similitud entre dos asignaciones de cluster (en este caso, tu variable PriceCategory y los clusters generados por K-means), ajustada por la posibilidad de coincidencia aleatoria. Los valores de ARI pueden variar entre -1 y 1, donde:

Un valor de 1 indica una coincidencia perfecta entre los dos agrupamientos.
Un valor de 0 indica una coincidencia no mejor que la aleatoria.
Un valor negativo indica una coincidencia peor que la aleatoria.
En tu caso, un ARI de aproximadamente 0.07 sugiere que hay una pequeña coincidencia entre los clusters generados por K-means y la variable PriceCategory, pero esta coincidencia es ligeramente mejor que la aleatoria. Sin embargo, no es una coincidencia fuerte, lo que indica que la agrupación realizada por K-means no se alinea de manera significativa con las categorías de precios existentes.
